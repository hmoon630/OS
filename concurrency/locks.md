# 락

# 락: 기본 개념

예를 위해 다음의 임계 영역이 있다고 하자.

```c
balance = balance + 1;
```

락을 사용하기 위해 락으로 임계 영역을 다음과 같이 감쌌다.

```c
lock_t mutex;
...
lock(&mutex);
balance = balance + 1;
unlock(&mutex);
```

**락 변수**는 락의 상태를 나타낸다. 락은 둘 중 하나의 상태를 갖는다. **사용 가능(available)** 상태 (**unlocked** 또는 **free**) 혹은 **사용 중(acquired)** 상태이다.

이 락 자료구조에 락을 보유한 쓰레드에 대한 정보나 락을 대기하는 쓰레드들에 대한 정보를 저장할 수도 있다.

lock()과 unlock() 루틴은 간단하다. lock()을 호출했을 때 아무도 락을 가지고 있지 않다면 락을 획득하게 된다. 이렇게 락을 획득한 쓰레드를 락 **소유자(owner)** 라고 부른다. 다른 쓰레드가 lock()을 호출할 경우 락이 사용 중인 동안 lock() 함수는 리턴하지 않는다.

unlock()을 호출한다면 락은 이제 다시 사용가능한 상태가 된다. 만약 대기 중인 쓰레드가 있었다면 락이 변경된걸 인지하고 락을 획득하여 임계 영역으로 진입하게 된다.

# Pthread 락

쓰레드 간의 **상호 배제(mutual exclusion)** 기능을 제공하기 떄문에 락을 **mutex**라고 부른다.

POSIX에서는 락과 언락 함수에 락의 변수명을 인자로 전달한다. 이는 하나의 락을 사용하는 것이 아닌 여러 개의 락을 사용하기 때문이다.

# 락의 평가

락의 정상동작 여부 판단을 위하 ㄴ평가기준은 다음과 같다.

1. **상호 배제**: 락이 동작하였을 때 다른 쓰레드가 임계 영역으로의 진입을 허용해서는 안된다.
2. **공정성**: 쓰레드들이 락 획득에 대해 공정한 기회를 가져야 한다. 더 극단적으로 락을 얻지 못해 **굶주리는(starve)** 경우도 없어야 한다.
3. **성능**: 락 사용의 시간적 오버헤드를 평가해야 한다. 고려해야할 상황은 다음과 같다.
    - 단일 CPU에서 하나의 쓰레드만 실행 시
    - 단일 CPU에서 여러 쓰레드 실행 시
    - 멀티 CPU에서 락 경쟁시

# 인터럽트 제어

초창기 단일 프로세스 시스템에서는 상호 배제 지원을 위해 임계 영역 내에서는 인터럽트를 비활성화 하는 방법을 사용했다. 코드로 나타내면 다음과 같다.

```c
void lock() {
    DisableInterrupts();
}
void unlock() {
    EnableInterrupts();
}
```

이 방법의 장점은 단순하다는 것이다. 인터럽트가 발생하지 않는다면 다른 쓰레드가 중간에 끼어들지 않는다.

하지만 이 방법은 단점이 많다. 먼저 이 요청을 하는 쓰레드가 인터럽트를 활성/비활성화 할 수 있는 **특권(privileged)** 연산을 실행할 수 있도록 허가해야 한다. 또 이를 다른 목적으로 사용하지 않음을 신뢰할 수 있어야 한다.

두번째 단점은 멀티프로세서에는 적용 할 수 없다는 것이다. 여러 쓰레드가 여러 CPU에서 실행 중이라면 하나의 프로세서에서 인터럽트를 비활성화 하더라도 다른 프로세서는 임계 영역 진입을 막을 수 없다.

세번째는 장시간 인터럽트를 중단시키면 중요한 인터럽트를 놓칠 수 있다. 예를 들어 CPU가 저정장치 I/O를 마친 사실을 놓친다면 언제 그 프로세스를 깨울 수 있을까?

# 실패한 시도: 오직 load/store 명령어만 사용하기

하나의 플래그와 load/store로 구현하는 방법이 있다.

하지만 이 방법은 문제가 있는다. 하나는 제대로 작동하지 않는다는 것이고, 다른 하나는 성능에서 문제가 있다.

두 쓰레드가 동시에 임계 영역에 진입하기 전 플래그 값을 읽는다. 플래그가 0인 것을 보고 임계 영역에 진입하나, 플래그를 1로 바꾸기 전에 다른 쓰레드가 또 들어오게되면 두 개 이상의 쓰레드가 임계 영역에 진입하게 된다. 

성능 문제로 while(flag == 0) 과 같이 무한히 플래그 값을 검사할 경우 (**spin wait**) 다른 쓰레드가 락을 해제할 때까지 시간을 낭비한다. 이 방법은 단일 프로세서에서 손해가 매우 크다.

# Test-And-Set을 사용하여 스핀 락 구현하기

하드웨어 기법 중 가장 기본은 **test-and-set** 명령어 또는 **원자적 교체(atomic exchange)** 라고 알려진 명령어다. 기본적인 동작은 다음의 C코드와 같다.

```c
int TestAndSet(int *old_ptrl, int new) {
    int old = *old_ptr;
    *old_ptr = new;
    return old;
}
```

락의 값을 **검사(test)** 하고 새로운 값 으로 **설정(set)** 하는 동작을 원자적 연산으로 만듦으로써 오직 하나의 쓰레드만 락을 획득할 수 있게 만들었다. 단일 프로세서에서 스핀 락 방식을 제대로 사용하려면 선점형 스케줄러(preemptive scheduler)를 사용해야 한다.

# 스핀 락 평가

이전에 명시한 기준에 의해 락의 효율을 평가해 볼 수 있다. 락에서 가장 중요한 측면은 상호 배제의 **정확성**이다. 스핀 락은 임의의 시간에 하나의 쓰레드만이 임계 영역에 진입할 수 있다.

다음은 **공정성**이다. 대기 중인 쓰레드에게 스핀 락은 얼마나 공정적인가? 하지만 스핀 락은 어떠한 공정성도 보장해 줄 수 없다. while문에서 회전 중인 쓰레드는 경쟁에 밀려 계속 그 상태로 남아 있을 수 있다.

마지막은 **성능**이다. 스핀 락을 사용할 떄 지불한는 비용은 무엇인가? 이는 두 가지 경우로 나누어서 생각해 볼 수 있다.

단일 CPU의 경우 스핀 락의 성능 오버헤드가 상당히 클 수 있다. N - 1개의 쓰레드들이 락을 획득하려고  CPU 사이클을 낭비하면서 대기하게 된다.

반면에 CPU가 여러개인 경우 스핀 락은 꽤 합리적으로 동작한다. CPU1에서 락을 획득하고 CPU2에서 그것을 대기하는 것이라면 다른 프로세서에서 while문으로 사이클을 사용하는 것은 그렇게 많은 사이클을 사용하지 않게 된다.

# Compare-And-Swap

또 다른 하드웨어 기법은 SPARC의 **Compare-And-Swap** 혹은 x86의 **Compare-And-Exchange** 가 있다.

```c
int CompareAndSwap(int *ptr, int expected, int new) {
    int original = *ptr;
    if (original == expected)
        *ptr = new;
    return original;
}
```

이 방법은 Test-And-Set 방식과 매우 유사하다. 하지만 CompareAndSwap은 TestAndSet보다 강력한데, **대기없는 동기화(wait-free synchronization)** 주제를 다룰 때 이 루틴의 능력을 알게 될 것이다.

# Load-Linked 와 Store-Conditional

어떤 플랫폼은 임계 영역 진입 제어 함수를 제작하기 위핸 명령어 쌍을 제공한다. MIPS 구조에서는 **load-linked**와 **store-conditional** 명령어를 사용하여 락 자료구조를 만든다. 이 명령어에 대한 의사 코드는 다음과 같다.

```c
int LoadLinked(int *ptr) {
    return *ptr;
}
int StoreConditional(int *ptr, int value) {
    if (no update to *ptr since LoadLinked to this address) {
        *ptr = value;
        return 1; // success!
    } else {
        return 0; // failed to update
    }
}
```

load-linked 명령어는 일반 로드 명령어와 같은 역할을 한다. 실제 차이는 store-conditional 명령어에 있다. 동일한 주소에 다른 store가 없었을 경우만 저장을 성공한다. 성공한 경우 load-linked 값을 갱신한다.

lock 명령어를 표현해보면 다음과 같다.

```c
void lock(lock_t *lock) {
    while (LoadLinked(&lock->flag) ||
        ! StoreConditional(*lock->flag, 1))
    ;
} 
```

# Fetch-And-Add

마지막 하드웨어 기법으로 **Fetch-And-Add** 명령어이다. 원자적으로 특정 주소의 예전 값을 반환하면서 값을 증가시킨다.

```c
int FetchAndAdd(int *ptr) {
    int old = *ptr;
    *ptr = old + 1;
    return old;
}
```

이 명령어를 이용해서 **티켓 락**이라는 것을 만들었다. 하나의 쓰레드가 사용하기 위해 lock()을 호출하면 티켓(번호표)을 받는다. 다른 쓰레드가 또 호출하면 다음 번호가 적힌 티켓을 받게 될 것이다.

이미 사용중인 쓰레드가 unlock()을 호출하면 다음 번호를 가진 쓰레드가 실행하게 된다.

# 과도한 스핀

지금까지의 방법은 하드웨어 기반의 락이다. 이들은 제대로 동작하지만, 효율적이지 않다. 쓰레드가 락을 보유한 채로 인터럽트에 걸릴 경우 락을 기다리는 다른 쓰레드들은 하염없이 반복적으로 스핀하게 된다. 이 때문에 CPU 시간을 낭비하게 된다.

# 양보

이전 쓰레드가 락을 가진 채로 인터럽트에 걸린 상태라면 어떻게 할 것인가?

첫번째 방법으로 락이 해제되기 전까지 스핀하는 대신 CPU를 다른 쓰레드에게 양보하는 것이다. 실행 중인 상태를 **준비** 상태로 변환하는 것이다. 이 양보 동작은 **deschedule**이라고도 불린다.

# 큐의 사용: 스핀 대신 잠자기

다수의 쓰레드가 락을 대기 중일 경우 다음으로 락을 획득할 쓰레드를 명시하고자 한다.

이 방법에서는 Test-And-Set 개념을 락 대기 전용 큐와 함께 사용하여 더 효율적인 락을 만든다. 그리고 락을 획득할 대상을 제어하여 기아 현상을 피한다.

guard 변수를 통해 스핀락으로 flag와 큐를 보호한다. 하지만 이는 아주 잠깐 동안의 스핀만 기다리면 해결되기에 오버헤드는 적다. 스핀이 끝나고 락을 획득할 수 있다면 획득하고 끝나지만, 획득하지 않았다면 자신의 tid를 큐에 넣고 park()를 호출하여 잠자기에 들어간다.

마지막으로 이 코드는 경쟁조건이 발생할 수 있다. park()를 호출하기 전에 unpark()를 먼저 호출하면 깨울 프로세스가 없기 떄문에 쓰레드는 락을 해제한다. 하지만 이후에 park()에 들어가면 이 쓰레드를 깨울 방법이 없다. 이를 **wakeup/waiting race**라고 부른다.

이 문제를 해결하기 위해 setpark()를 도입하였다. 이는 쓰레드가 park()를 호출하기 직전임을 나타낸다. 만약 그때 인터럽트되어 unpark()가 먼저 실행되어도 park() 호출 시 이를 호출한 쓰레드는 바로 리턴된다.
